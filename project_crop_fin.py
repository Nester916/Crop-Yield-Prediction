# -*- coding: utf-8 -*-
"""project_crop_fin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sKRLPQT6E76ZO09K-arWulHw0llox8Nn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import scipy.stats as stats
import xgboost as xgb
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import griddata
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score
from sklearn.svm import SVR
import warnings
warnings.filterwarnings('ignore')
!pip install catboost
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error

# Importing dataset
file_link =

id = file_link.split("/")[-2]

new_link = f'https://drive.google.com/uc?id={id}'
df = pd.read_csv(new_link)

print("Shape of the dataset : ",df.shape)

df.isnull().sum()

rows_with_multiple_missing = df[df[['Annual_Rainfall', 'State', 'Season']].isnull().sum(axis=1) >= 2].index

# Drop rows with at least two missing values among 'Annual_Rainfall', 'State', and 'Season'
df.drop(index=rows_with_multiple_missing, inplace=True)

average_rainfall_by_state = df.groupby('State')['Annual_Rainfall'].mean()
def find_closest_state(row):
    if pd.isnull(row['State']):
        other_states_avg = average_rainfall_by_state.reset_index(drop=True)
        diff = np.abs(average_rainfall_by_state - row['Annual_Rainfall'])
        closest_state = diff.idxmin()
        return closest_state
    else:
        return row['State']

df['State'] = df.apply(find_closest_state, axis=1)

missing_area_rows = df['Area'].isnull()
df.loc[missing_area_rows, 'Area'] = df.loc[missing_area_rows, 'Production'] / df.loc[missing_area_rows, 'Yield']

missing_production_rows = df['Production'].isnull()
df.loc[missing_production_rows, 'Production'] = df.loc[missing_production_rows, 'Yield'] * df.loc[missing_production_rows, 'Area']

average_rainfall_by_state = df.groupby('State')['Annual_Rainfall'].mean()
def fill_missing_rainfall(row):
    if pd.isnull(row['Annual_Rainfall']):
        return average_rainfall_by_state[row['State']]
    else:
        return row['Annual_Rainfall']

df['Annual_Rainfall'] = df.apply(fill_missing_rainfall, axis=1)

average_fertilizer_by_crop = df.groupby('Crop')['Fertilizer'].mean()
average_pesticide_by_crop = df.groupby('Crop')['Pesticide'].mean()

def fill_missing_values(row):
    if pd.isnull(row['Fertilizer']):
        return average_fertilizer_by_crop[row['Crop']]
    elif pd.isnull(row['Pesticide']):
        return average_pesticide_by_crop[row['Crop']]
    else:
        return row['Fertilizer'], row['Pesticide']

df[['Fertilizer', 'Pesticide']] = df.apply(fill_missing_values, axis=1, result_type='expand')

average_season_by_crop = df.groupby('Crop')['Season'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)

# Function to fill missing 'Season' values with average season for the corresponding crop
def fill_missing_season(row):
    if pd.isnull(row['Season']):
        return average_season_by_crop[row['Crop']]
    else:
        return row['Season']

# Apply the function to fill missing 'Season' values
df['Season'] = df.apply(fill_missing_season, axis=1)

df.isnull().sum()

df.nunique()

df.describe(include='all')

cat_cols=df.select_dtypes(include=['object']).columns.tolist()
num_cols = df.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

# Grouping by year and crop and calculating the average yield and annual rainfall
grouped_df = df.groupby(['Crop', 'Crop_Year']).agg({'Yield': 'mean', 'Annual_Rainfall': 'mean'}).reset_index()

# Get unique crop names
crops = grouped_df['Crop'].unique()

# Set up the plots for each crop
for crop in crops:
    crop_data = grouped_df[grouped_df['Crop'] == crop]

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # Generating a meshgrid for continuous representation with exchanged axes
    y = np.linspace(crop_data['Annual_Rainfall'].min(), crop_data['Annual_Rainfall'].max(), 100)
    x = crop_data['Crop_Year']
    Y, X = np.meshgrid(y, x)

    # Interpolating Yield values for the meshgrid points
    Z = griddata((crop_data['Crop_Year'], crop_data['Annual_Rainfall']), crop_data['Yield'], (X, Y), method='linear')

    # Plotting the surface
    surf = ax.plot_surface(X, Y, Z, cmap='viridis')

    # Setting labels and title
    ax.set_xlabel('Year')
    ax.set_ylabel('Average Annual Rainfall')
    ax.set_zlabel('Average Yield')
    ax.set_title(f'{crop} - Year vs Average Rainfall vs Yield')

    # Setting axis limits based on the filtered statistics
    ax.set_xlim(crop_data['Crop_Year'].min(), crop_data['Crop_Year'].max())
    ax.set_ylim(crop_data['Annual_Rainfall'].min(), crop_data['Annual_Rainfall'].max())
    ax.set_zlim(crop_data['Yield'].min(), crop_data['Yield'].max())

    # Adding a color bar
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()

# Grouping by crop and state and calculating the average yield and annual rainfall
grouped_df = df.groupby(['Crop', 'State']).agg({'Yield': 'mean', 'Annual_Rainfall': 'mean'}).reset_index()

# Encode state names to numeric values
state_encoder = LabelEncoder()
grouped_df['State_Code'] = state_encoder.fit_transform(grouped_df['State'])

# Get unique crop names
crops = grouped_df['Crop'].unique()

# Set up the plots for each crop
for crop in crops:
    crop_data = grouped_df[grouped_df['Crop'] == crop]

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # Generating a meshgrid for continuous representation with exchanged axes
    y = np.linspace(crop_data['Annual_Rainfall'].min(), crop_data['Annual_Rainfall'].max(), 100)
    x = crop_data['State_Code']
    Y, X = np.meshgrid(y, x)

    # Interpolating Yield values for the meshgrid points
    Z = griddata((crop_data['State_Code'], crop_data['Annual_Rainfall']), crop_data['Yield'], (X, Y), method='linear')

    # Plotting the surface
    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)

    # Adding wireframe
    ax.plot_wireframe(X, Y, Z, color='black', linewidth=0.5)

    # Setting labels and title
    ax.set_xlabel('State')
    ax.set_ylabel('Average Annual Rainfall')
    ax.set_zlabel('Average Yield')
    ax.set_title(f'{crop} - State vs Average Rainfall vs Yield')

    # Setting axis limits based on the filtered statistics
    ax.set_xlim(crop_data['State_Code'].min(), crop_data['State_Code'].max())
    ax.set_ylim(crop_data['Annual_Rainfall'].min(), crop_data['Annual_Rainfall'].max())
    ax.set_zlim(crop_data['Yield'].min(), crop_data['Yield'].max())

    # Adding a color bar
    fig.colorbar(surf, shrink=0.5, aspect=5)

    # Set tick labels for the state axis
    ax.set_xticks(crop_data['State_Code'].unique())
    ax.set_xticklabels(state_encoder.inverse_transform(crop_data['State_Code'].unique()), rotation=45, ha='right')

    plt.show()

# Filter the DataFrame for rice crop
rice_df = df[df['Crop'] == 'Rice']

# Scatter plot for Pesticide vs Yield
plt.figure(figsize=(10, 6))
plt.scatter(rice_df['Pesticide'], rice_df['Yield'], alpha=0.5)
plt.title('Pesticide vs Yield for Rice')
plt.xlabel('Pesticide')
plt.ylabel('Yield')
plt.grid(True)
plt.show()

# Scatter plot for Fertilizer vs Yield
plt.figure(figsize=(10, 6))
plt.scatter(rice_df['Fertilizer'], rice_df['Yield'], alpha=0.5)
plt.title('Fertilizer vs Yield for Rice')
plt.xlabel('Fertilizer')
plt.ylabel('Yield')
plt.grid(True)
plt.show()

# Calculate average rainfall for each state
average_rainfall_per_state = df.groupby('State')['Annual_Rainfall'].mean()

# Sort states based on average rainfall
sorted_states = average_rainfall_per_state.sort_values().index

# Create a mapping of states to their sorted order
state_order_mapping = {state: i for i, state in enumerate(sorted_states)}

# Reassign state codes based on the sorted order
df['State_Code'] = df['State'].map(state_order_mapping)

# Group by state code and year and calculate the average annual rainfall
grouped_df = df.groupby(['State_Code', 'Crop_Year'])['Annual_Rainfall'].mean().reset_index()

# Find the highest rainfall value
max_rainfall = grouped_df['Annual_Rainfall'].max()

# Generate meshgrid for continuous representation
x = np.arange(len(sorted_states))  # State
y = np.arange(grouped_df['Crop_Year'].min(), grouped_df['Crop_Year'].max() + 1)  # Year
X, Y = np.meshgrid(x, y)

# Interpolate Annual Rainfall values for the meshgrid points
Z = griddata((grouped_df['State_Code'], grouped_df['Crop_Year']), grouped_df['Annual_Rainfall'], (X, Y), method='linear')

# Plot the mesh
fig = plt.figure(figsize=(16, 15))
ax = fig.add_subplot(111, projection='3d')

# Plotting the surface
surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')

# Setting labels and title
ax.set_xlabel('State')
ax.set_ylabel('Year')
ax.set_zlabel('Annual Rainfall (mm)')
ax.set_title('Annual Rainfall vs State vs Year')

# Set the z-axis limit to the highest rainfall value
ax.set_zlim(0, max_rainfall)

# Set custom tick labels for the state axis with increased spacing
ax.set_xticks(np.arange(len(sorted_states)))
ax.set_xticklabels(sorted_states, rotation=45, ha='right')

# Make the graph more horizontal
ax.view_init(elev=20, azim=150)

# Adding a color bar
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()

fertilizer = df['Fertilizer']
pesticide = df['Pesticide']

# Creating the scatter plot with limited axis values
plt.figure(figsize=(8, 6))
plt.scatter(fertilizer, pesticide, alpha=0.5)
plt.title('Fertilizer vs Pesticide')
plt.xlabel('Fertilizer')
plt.ylabel('Pesticide')
plt.grid(True)
plt.show()

# Calculating the correlation coefficient
correlation_coefficient = fertilizer.corr(pesticide)
print("Correlation Coefficient between Fertilizer and Pesticide:", correlation_coefficient)

df = df.drop(['State_Code'], axis = 1)

numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Correlation matrix plot
plt.figure(figsize=(8, 8))
correlation_matrix = numeric_df.corr()
plt.matshow(correlation_matrix)
plt.title('Correlation Matrix')
plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)
plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
plt.colorbar()
plt.show()

df_encoded = pd.get_dummies(df, columns=['Crop', 'Season', 'State'], drop_first=True)
df_encoded.describe(include='all')

df_encoded.shape

df_encoded = df_encoded.drop(['Production'], axis = 1)

df_encoded_0 = df_encoded.drop(['Crop_Year','Fertilizer','Pesticide'], axis = 1)
df_encoded_1 = df_encoded.drop(['Crop_Year','Pesticide'], axis = 1)
df_encoded_2 = df_encoded.drop(['Crop_Year','Fertilizer'], axis = 1)

df_encoded_0.shape

df_encoded_1.shape

plt.figure(figsize=(15,26))
plt.subplot(4,2,1)
sns.distplot(df_encoded_0['Area'],bins = 20,color = 'red')
plt.subplot(4,2,2)
stats.probplot(df_encoded_0['Area'], dist = 'norm', plot = plt)
plt.subplot(4,2,3)
sns.distplot(df_encoded_0['Annual_Rainfall'],bins = 20,color = 'blue')
plt.subplot(4,2,4)
stats.probplot(df_encoded_0['Annual_Rainfall'], dist = 'norm', plot = plt)
plt.subplot(4,2,5)
sns.distplot(df_encoded_1['Fertilizer'],bins = 20, color = 'black')
plt.subplot(4,2,6)
stats.probplot(df_encoded_1['Fertilizer'], dist = 'norm', plot = plt)
plt.subplot(4,2,7)
sns.distplot(df_encoded_2['Pesticide'],bins = 20, color = 'purple')
plt.subplot(4,2,8)
stats.probplot(df_encoded_2['Pesticide'], dist = 'norm', plot = plt)
plt.show()

x_train, x_test, y_train,y_test = train_test_split(df_encoded.drop(['Yield'], axis = 1), df_encoded[['Yield']], test_size = 0.3, random_state = 42)
x0_train, x0_test, y0_train,y0_test = train_test_split(df_encoded_0.drop(['Yield'], axis = 1), df_encoded_0[['Yield']], test_size = 0.3, random_state = 42)
x1_train, x1_test, y1_train,y1_test = train_test_split(df_encoded_1.drop(['Yield'], axis = 1), df_encoded_1[['Yield']], test_size = 0.3, random_state = 42)
x2_train, x2_test, y2_train,y2_test = train_test_split(df_encoded_2.drop(['Yield'], axis = 1), df_encoded_2[['Yield']], test_size = 0.3, random_state = 42)

from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler, RobustScaler, Normalizer, QuantileTransformer, FunctionTransformer
from scipy.stats import boxcox
#PowerTransformer - 1
pt = PowerTransformer(method='yeo-johnson')
x_train_transform1 = pt.fit_transform(x_train)
x_test_transform1 = pt.fit_transform(x_test)
df_trans1 = pd.DataFrame(x_train_transform1, columns=x_train.columns)
x0_train_transform1 = pt.fit_transform(x0_train)
x0_test_transform1 = pt.fit_transform(x0_test)
df0_trans1 = pd.DataFrame(x0_train_transform1, columns=x0_train.columns)
x1_train_transform1 = pt.fit_transform(x1_train)
x1_test_transform1 = pt.fit_transform(x1_test)
df1_trans1 = pd.DataFrame(x1_train_transform1, columns=x1_train.columns)
x2_train_transform1 = pt.fit_transform(x2_train)
x2_test_transform1 = pt.fit_transform(x2_test)
df2_trans1 = pd.DataFrame(x2_train_transform1, columns=x2_train.columns)
# MinMaxScaler - 3
min_max_scaler = MinMaxScaler()
x_train_transform3 = min_max_scaler.fit_transform(x_train)
x_test_transform3 = min_max_scaler.fit_transform(x_test)
df_trans3 = pd.DataFrame(x_train_transform3, columns=x_train.columns)
x0_train_transform3 = min_max_scaler.fit_transform(x0_train)
x0_test_transform3 = min_max_scaler.fit_transform(x0_test)
df0_trans3 = pd.DataFrame(x0_train_transform3, columns=x0_train.columns)
x1_train_transform3 = min_max_scaler.fit_transform(x1_train)
x1_test_transform3 = min_max_scaler.fit_transform(x1_test)
df1_trans3 = pd.DataFrame(x1_train_transform3, columns=x1_train.columns)
x2_train_transform3 = min_max_scaler.fit_transform(x2_train)
x2_test_transform3 = min_max_scaler.fit_transform(x2_test)
df2_trans3 = pd.DataFrame(x2_train_transform3, columns=x2_train.columns)
# RobustScaler - 4
robust_scaler = RobustScaler()
x_train_transform4 = robust_scaler.fit_transform(x_train)
x_test_transform4 = robust_scaler.fit_transform(x_test)
df_trans4 = pd.DataFrame(x_train_transform4, columns=x_train.columns)
x0_train_transform4 = robust_scaler.fit_transform(x0_train)
x0_test_transform4 = robust_scaler.fit_transform(x0_test)
df0_trans4 = pd.DataFrame(x0_train_transform4, columns=x0_train.columns)
x1_train_transform4 = robust_scaler.fit_transform(x1_train)
x1_test_transform4 = robust_scaler.fit_transform(x1_test)
df1_trans4 = pd.DataFrame(x1_train_transform4, columns=x1_train.columns)
x2_train_transform4 = robust_scaler.fit_transform(x2_train)
x2_test_transform4 = robust_scaler.fit_transform(x2_test)
df2_trans4 = pd.DataFrame(x2_train_transform4, columns=x2_train.columns)
# Log Transformation - 7
x_train_transform7 = np.log(x_train+1)
x_test_transform7 = np.log(x_test+1)
df_trans7 = pd.DataFrame(x_train_transform7, columns=x_train.columns)
x0_train_transform7 = np.log(x0_train+1)
x0_test_transform7 = np.log(x0_test+1)
df0_trans7 = pd.DataFrame(x0_train_transform7, columns=x0_train.columns)
x1_train_transform7 = np.log(x1_train+1)
x1_test_transform7 = np.log(x1_test+1)
df1_trans7 = pd.DataFrame(x1_train_transform7, columns=x1_train.columns)
x2_train_transform7 = np.log(x2_train+1)
x2_test_transform7 = np.log(x2_test+1)
df2_trans7 = pd.DataFrame(x2_train_transform7, columns=x2_train.columns)

plt.figure(figsize=(15,26))
plt.subplot(5,2,1)
sns.distplot(df0_trans7['Area'],bins = 20,color = 'red')
plt.subplot(5,2,2)
stats.probplot(df0_trans7['Area'], dist = 'norm', plot = plt)
plt.subplot(5,2,3)
sns.distplot(df0_trans7['Annual_Rainfall'],bins = 20,color = 'blue')
plt.subplot(5,2,4)
stats.probplot(df0_trans7['Annual_Rainfall'], dist = 'norm', plot = plt)
plt.subplot(5,2,5)
sns.distplot(df1_trans7['Fertilizer'],bins = 20, color = 'black')
plt.subplot(5,2,6)
stats.probplot(df1_trans7['Fertilizer'], dist = 'norm', plot = plt)
plt.subplot(5,2,7)
sns.distplot(df2_trans7['Pesticide'],bins = 20, color = 'purple')
plt.subplot(5,2,8)
stats.probplot(df2_trans7['Pesticide'], dist = 'norm', plot = plt)
plt.show()

train_r2=[]
test_r2=[]

# Linear Regression
lr = LinearRegression()
lr.fit(x_train, y_train)
y_pred_train= lr.predict(x_train)
y_pred_test= lr.predict(x_test)
print("Training Accuracy: ",r2_score(y_train, y_pred_train))
print("Test Accuracy: ",r2_score(y_test, y_pred_test))
print('########################################################')
lr.fit(x_train_transform4, y1_train)
y_pred_train_4 = lr.predict(x_train_transform4)
y_pred_test_4 = lr.predict(x_test_transform4)
print("Training Accuracy (Transform4 - LR): ",r2_score(y_train, y_pred_train_4))
print("Test Accuracy (Transform4 - LR): ",r2_score(y_test, y_pred_test_4))
print("Training Precision (Transform4 - LR): ", np.sqrt(mean_squared_error(y_train, y_pred_train_4)))
print("Test Precision (Transform4 - LR): ", np.sqrt(mean_squared_error(y_test, y_pred_test_4)))
train_r2+=[r2_score(y_train, y_pred_train_4)]
test_r2+=[r2_score(y_test, y_pred_test_4)]

# Create and configure the neural network model
nn_model = MLPRegressor(hidden_layer_sizes=(100, 50),
                        activation='relu',
                        solver='adam',
                        max_iter=500,
                        random_state=42)

nn_model.fit(x_train, y_train)
y_pred_train_nn = nn_model.predict(x_train)
y_pred_test_nn = nn_model.predict(x_test)
print("Training Accuracy (NN, MLPR): ", r2_score(y_train, y_pred_train_nn))
print("Test Accuracy (NN, MLPR): ", r2_score(y_test, y_pred_test_nn))
print('########################################################')
nn_model.fit(x_train_transform3, y_train)
y_pred_train_nn = nn_model.predict(x_train_transform3)
y_pred_test_nn = nn_model.predict(x_test_transform3)
print("Training Accuracy (Transform3 - NN, MLPR): ", r2_score(y_train, y_pred_train_nn))
print("Test Accuracy (Transform3 - NN, MLPR): ", r2_score(y_test, y_pred_test_nn))
print("Training Precision (Transform3 - NN, MLPR): ", np.sqrt(mean_squared_error(y_train, y_pred_train_nn)))
print("Test Precision (Transform3 - NN, MLPR): ", np.sqrt(mean_squared_error(y_test, y_pred_test_nn)))
train_r2+=[r2_score(y_train, y_pred_train_nn)]
test_r2+=[r2_score(y_test, y_pred_test_nn)]

# Create SGDRegressor for linear regression
sgd_regressor = SGDRegressor(learning_rate='constant', eta0=0.0012383, max_iter=3000, random_state=42)

# Fit and evaluate on second set of transformed features
sgd_regressor.fit(x_train_transform7, y_train)
y_pred_train_sdgr = sgd_regressor.predict(x_train_transform7)
y_pred_test_sdgr = sgd_regressor.predict(x_test_transform7)
print("Training Accuracy (Transform7 - SDGR): ", r2_score(y_train, y_pred_train_sdgr))
print("Test Accuracy (Transform7 - SDGR): ", r2_score(y_test, y_pred_test_sdgr))
print('########################################################')
sgd_regressor.fit(x2_train_transform7, y2_train)
y2_pred_train_sdgr = sgd_regressor.predict(x2_train_transform7)
y2_pred_test_sdgr = sgd_regressor.predict(x2_test_transform7)
print("Training Accuracy (Transform7 - SDGR): ", r2_score(y2_train, y2_pred_train_sdgr))
print("Test Accuracy (Transform7 - SDGR): ", r2_score(y2_test, y2_pred_test_sdgr))
print('########################################################')
sgd_regressor.fit(x0_train_transform7, y0_train)
y0_pred_train_sdgr = sgd_regressor.predict(x0_train_transform7)
y0_pred_test_sdgr = sgd_regressor.predict(x0_test_transform7)
print("Training Accuracy (Transform7 - SDGR): ", r2_score(y0_train, y0_pred_train_sdgr))
print("Test Accuracy (Transform7 - SDGR): ", r2_score(y0_test, y0_pred_test_sdgr))
print("Training Precision (Transform7 - SDGR): ", np.sqrt(mean_squared_error(y0_train, y0_pred_train_sdgr)))
print("Test Precision (Transform7 - SDGR): ", np.sqrt(mean_squared_error(y0_test, y0_pred_test_sdgr)))
############################################################################################
train_r2+=[r2_score(y0_train, y0_pred_train_sdgr)]
test_r2+=[r2_score(y0_test, y0_pred_test_sdgr)]

# Create XGBoost regressor
xgb_regressor = xgb.XGBRegressor()
xgb_regressor.fit(x_train, y_train)
y_pred_train_xgb = xgb_regressor.predict(x_train)
y_pred_test_xgb = xgb_regressor.predict(x_test)
print("Training Accuracy (XGBR): ", r2_score(y_train, y_pred_train_xgb))
print("Test Accuracy (XGBR): ", r2_score(y_test, y_pred_test_xgb))
print("Training Precision (XGBR): ", np.sqrt(mean_squared_error(y_train, y_pred_train_xgb)))
print("Test Precision (XGBR): ", np.sqrt(mean_squared_error(y_test, y_pred_test_xgb)))
train_r2+=[r2_score(y_train, y_pred_train_xgb)]
test_r2+=[r2_score(y_test, y_pred_test_xgb)]

dtr = DecisionTreeRegressor()
dtr.fit(x_train, y_train)
y_pred_train_dt = dtr.predict(x_train)
y_pred_test_dt = dtr.predict(x_test)
print("Training Accuracy (Decision Tree): ", r2_score(y_train, y_pred_train_dt))
print("Test Accuracy (Decision Tree): ", r2_score(y_test, y_pred_test_dt))
print('########################################################')
dtr.fit(x2_train, y2_train)
y2_pred_train_dt = dtr.predict(x2_train)
y2_pred_test_dt = dtr.predict(x2_test)
print("Training Accuracy (Decision Tree): ", r2_score(y2_train, y2_pred_train_dt))
print("Test Accuracy (Decision Tree): ", r2_score(y2_test, y2_pred_test_dt))
print('########################################################')
dtr.fit(x2_train_transform1, y2_train)
y2_pred_train_dt = dtr.predict(x2_train_transform1)
y2_pred_test_dt = dtr.predict(x2_test_transform1)
print("Training Accuracy (Transform1 - Decision Tree): ", r2_score(y2_train, y2_pred_train_dt))
print("Test Accuracy (Transform1 - Decision Tree): ", r2_score(y2_test, y2_pred_test_dt))
print("Training Precision (Transform1 - Decision Tree): ", np.sqrt(mean_squared_error(y2_train, y2_pred_train_dt)))
print("Test Precision (Transform1 - Decision Tree): ", np.sqrt(mean_squared_error(y2_test, y2_pred_test_dt)))
train_r2+=[r2_score(y2_train, y2_pred_train_dt)]
test_r2+=[r2_score(y2_test, y2_pred_test_dt)]

# Create RandomForest for Regression
regr = RandomForestRegressor()
regr.fit(x_train, y_train)
y_pred_train_dt = regr.predict(x_train)
y_pred_test_dt = regr.predict(x_test)
print("Training Accuracy - (RFR): ", r2_score(y_train, y_pred_train_dt))
print("Test Accuracy - (RFR): ", r2_score(y_test, y_pred_test_dt))

print('########################################################')

regr.fit(x2_train_transform1, y2_train)
y2_pred_train_regr= regr.predict(x2_train_transform1)
y2_pred_test_regr = regr.predict(x2_test_transform1)
print("Training Accuracy (Transform1 - RFR): ",r2_score(y2_train, y2_pred_train_regr))
print("Test Accuracy (Transform1 - RFR): ",r2_score(y2_test, y2_pred_test_regr))
print("Training Precision (Transform1 - RFR): ", np.sqrt(mean_squared_error(y2_train, y2_pred_train_regr)))
print("Test Precision (Transform1 - RFR): ", np.sqrt(mean_squared_error(y2_test, y2_pred_test_regr)))
train_r2+=[r2_score(y2_train, y2_pred_train_regr)]
test_r2+=[r2_score(y2_test, y2_pred_test_regr)]

# Create KNeighborsRegressor
knn_regressor = KNeighborsRegressor(n_neighbors=4)  # You can adjust the number of neighbors (k)
# Fit and evaluate on second set of transformed features
knn_regressor.fit(x_train, y_train)
y_pred_train_knn = knn_regressor.predict(x_train)
y_pred_test_knn = knn_regressor.predict(x_test)
print("Training R2 Score (KNR): ", r2_score(y_train, y_pred_train_knn))
print("Test R2 Score (KNR): ", r2_score(y_test, y_pred_test_knn))
print('########################################################')

knn_regressor.fit(x_train_transform7, y_train)
y_pred_train_knn = knn_regressor.predict(x_train_transform7)
y_pred_test_knn = knn_regressor.predict(x_test_transform7)
print("Training R2 Score (KNR): ", r2_score(y_train, y_pred_train_knn))
print("Test R2 Score (KNR): ", r2_score(y_test, y_pred_test_knn))
print('########################################################')

knn_regressor.fit(x1_train_transform7, y1_train)
y1_pred_train_knn = knn_regressor.predict(x1_train_transform7)
y1_pred_test_knn = knn_regressor.predict(x1_test_transform7)
print("Training R2 Score (Transform7 - KNR): ", r2_score(y1_train, y1_pred_train_knn))
print("Test R2 Score (Transform7 - KNR): ", r2_score(y1_test, y1_pred_test_knn))
print("Training Precision (Transform7 - KNR): ", np.sqrt(mean_squared_error(y1_train, y1_pred_train_knn)))
print("Test Precision (Transform7 - KNR): ", np.sqrt(mean_squared_error(y1_test, y1_pred_test_knn)))
train_r2+=[r2_score(y1_train, y1_pred_train_knn)]
test_r2+=[r2_score(y1_test, y1_pred_test_knn)]

# Create SVR model
svr_regressor = SVR(kernel='rbf')  # 'rbf' kernel is commonly used for non-linear regression
# Fit and evaluate on second set of transformed features
svr_regressor.fit(x0_train_transform1, y_train)
y_pred_train_svr = svr_regressor.predict(x0_train_transform1)
y_pred_test_svr = svr_regressor.predict(x0_test_transform1)
print("Training R2 Score (Transform1 - SVR): ", r2_score(y_train, y_pred_train_svr))
print("Test R2 Score (Transform1 - SVR): ", r2_score(y_test, y_pred_test_svr))
print("Training Precision (Transform1 - SVR): ", np.sqrt(mean_squared_error(y_train, y_pred_train_svr)))
print("Test Precision (Transform1 - SVR): ", np.sqrt(mean_squared_error(y_test, y_pred_test_svr)))
train_r2+=[r2_score(y_train, y_pred_train_svr)]
test_r2+=[r2_score(y_test, y_pred_test_svr)]

# Create CatBoostRegressor
catboost_regressor = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE')
# Fit and evaluate on second set of transformed features
catboost_regressor.fit(x_train, y_train, verbose=100)  # Adjust verbosity level if needed
y_pred_train_catboost = catboost_regressor.predict(x_train)
y_pred_test_catboost = catboost_regressor.predict(x_test)
print("Training R2 Score (CatBoost): ", r2_score(y_train, y_pred_train_catboost))
print("Test R2 Score (CatBoost): ", r2_score(y_test, y_pred_test_catboost))
print('########################################################')
catboost_regressor.fit(x_train_transform3, y_train, verbose=100)  # Adjust verbosity level if needed
y_pred_train_catboost = catboost_regressor.predict(x_train_transform3)
y_pred_test_catboost = catboost_regressor.predict(x_test_transform3)
print("Training R2 Score (Transform7 - CatBoost): ", r2_score(y_train, y_pred_train_catboost))
print("Test R2 Score (Transform7 - CatBoost): ", r2_score(y_test, y_pred_test_catboost))
print("Training Precision (Transform7 - CatBoost): ", np.sqrt(mean_squared_error(y_train, y_pred_train_catboost)))
print("Test Precision (Transform7 - CatBoost): ", np.sqrt(mean_squared_error(y_test, y_pred_test_catboost)))
train_r2+=[r2_score(y_train, y_pred_train_catboost)]
test_r2+=[r2_score(y_test, y_pred_test_catboost)]

ml_algs=['Linear Regression', 'Neural Network', 'Stochastic Gradient Descent', 'Extreme Gradient Boosting', 'Decision Tree', 'Random Forest', 'K- Nearest Neighbors', 'Support Vector Regression', 'CatBoost Regressor']
conf_data={'Training R2 Score':train_r2, 'Testing R2 Score':test_r2}
table_0=pd.DataFrame(conf_data, index=ml_algs)
table_0

conf_data = {
    'Training R2 Score': train_r2,
    'Testing R2 Score': test_r2
}

ml_algs=['Linear Regression', 'Neural Network', 'Stochastic Gradient Descent',
         'Extreme Gradient Boosting', 'Decision Tree', 'Random Forest',
         'K- Nearest Neighbors', 'Support Vector Regression', 'CatBoost Regressor']

table_0=pd.DataFrame(conf_data, index=ml_algs)

# Plotting
plt.figure(figsize=(10, 6))
table_0.plot(kind='bar', ax=plt.gca())
plt.title('R2 Score Comparison for Different Algorithms')
plt.ylabel('R2 Score')
plt.xlabel('Algorithms')
plt.xticks(rotation=45, ha='right')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()